### 第一章 绪论

#### 1.2 基本术语

数据集（data set）

​	-- 每条记录 称为 **示例（instance）**或**样本（sample）**

​		-- 样本上的表现或性质的项 称为 **属性（attribute）**或**特征（feature）**

​			-- 属性上的值	称为 **属性值**

​			-- 属性的数量值 称为 **维数**

​			-- 属性张成的空间称为 **属性空间（attribute space**）、**样本空间（sample space）**或**输入空间**

​			-- 把空间中的点对应的坐标向量，称为**特征向量（feature vector）**

从数据中学的模型的过程称为 **学习（learning）**或**训练（trainning）**

学的模型后，使用其进行预测的过程称为 **测试**（testing），被预测的样本称为**测试样本**（testing sample）

根据训练数据是否拥有标记信息，学习任务可大致划分为两大类: “**监督学习**“(supervised learning)和 “**无监督学习**"(unsupervised learning),分类 和回归是前者的代表,而聚类则是后者的代表.

学的模型适用于新样本的能力，称为 **泛化能力**。

通常假设样本空间中全 体样本服从一个未知**“分布"(distribution)**我们获得的每个样本都是独立 地从这个分布上采样获得的，即 “**独立同分布**" (independent and identically distributed,简称i.i.d）





#### 1.3 假设空间

**归纳(induction)**与**演绎(deduction)**是科学推理的两大基本手段.前者是从 特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规 律;后者则是从一般到特殊的“特化”(specialization)过程，即从基础原理推演出具体状况

归纳学习有狭义与广义之分，广义的归纳学习大体相当于从样例中学习, 而狭义的归纳学习则要求从训练数据中学得概念(concept),因此亦称为“**概念学习**”或 “概念形成”。概念学习中最基本的是布尔概念学习。

我们可以把学习过程看作一个在所有假设(hypothesis)组成的空间中进行 搜索的过程,搜索目标是找到与训练集“匹配”(班)的假设，即能够将训练集中 的瓜判断正确的假设.





#### 1.4 归纳偏好

归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进 行选择的启发式或“价值观”



假设样本空间 $X$ 和假设空间 $H$ 都是离散的. 

令 $P(h \mid X,\zeta_a)$  代表算法 $\zeta_a$ 基于训练数据 $X$ 产生假设 $h$ 的概率，再令 $f$ 代表我们希望学习的真实目标函数.

$\zeta_a$ 的训练集外误差，即 $\zeta_a$ 在训练集之外的所有样本上的误差为

​		$E_{ote}(\zeta_a \mid X, f) = \sum_h \sum_{x \in \mathcal{x} \sim X} P(x)\prod(h(x) \ne f(x)) P(h \mid X, \zeta_a), 其中\prod(.)是指示函数，若.为真则取值1，否则取值0.$ 

考虑二分类问题，且真实目标函数可以是任何函数 X->{0,1}, 函数空间为 $\{0,1\}^{|X|}$ 

// todo



**没有免费的午餐**NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义。





#### 1.5 发展历程

二十世纪八十年代，“从样例中学习”的一大主流是符号主义学习, 其代表包括决策树(decision tree)和基于逻辑的学习.

二十世纪九十年代中期之前，“从样例中学习”的另一主流技术是基于神 经网络的连接主义学习

二十世纪九十年代中期，“统计学习”(statistical learning)闪亮登场并 迅速占据主流舞台，代表性技术是支持向量机(Support Vector Machine,简称 SVM)以及更一般的“核方法”(kernel methods

二十一世纪初，连接主义学习又卷土重来，掀起了以“深度学习” 为名的热潮.所谓深度学习，狭义地说就是“很多层”的神经网络.

统计学主要是通过机器学习对数据挖 掘发挥影响，而机器学习领域和数据库领域则是数据挖掘的两大支撑。



**不显式编程地赋予计算机能力的研究领域** 我喜欢这个词



