### 第二章 模型评估与选择

#### 2.1 经验误差与过拟合

**错误率（error rate）** 分类错误的样本数占样本总数的比例

**精度（accuracy）** 即 精度 = 1 - 错误率

**误差（error）** 我们把学习器的实际预测输出与样本的真实输出之间的差异

**训练误差（training error）或经验误差（empirical error）** 学习器在训练集上的误差

**泛化误差（generalization error）** 学习器在新样本上的误差



显然，我们希望得到泛化误差小的学习器.然而，我们事先并不知道新样本是什么样，实际能做的是努力使经验误差最小化。

**欠拟合**比较容易克服，例如在决策树学习中扩展分支、在神经网络学习中增加训练轮数等，而**过拟合**则很麻烦.在后面的学习中 我们将看到，过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一 些针对过拟合的措施;





#### 2.2 评估方法

以**测试集（testing set）**来测试学习器对新样本的判别能力，然后以测试集上的**测试误差（testing error）**做为泛化误差的近似。需注意的是测试集应该尽可能与测试集互斥，即测试样本尽量不在训练集中出现使用。



##### 2.2.1 **留出法**

**留出法（hold-out）**直接将数据集划分为两个互斥的集合。

**分层采样（stratified sampling）** 保留类别比例的采样方式。例如通过对D 进行分层采样而获得含70%样本的训练集S 和含 30%样本的测试集T,若D 包含500个正例、500个反例，则分层采样得到的 S 应包含350个正例、350个反例，而T 则包含150个正例和150个反例;若 S、T 中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异 而产生偏差.



##### 2.2.2 交叉验证法

**交叉验证法（cross validation）** 先将数据集D 划分为k 个大小相似的 互斥子集，然后，每次用 k - 1 个子集的并集作为训练集，余下的那个子集作为测试集;这样就可获得k 组训练/测试集，从而可进行k 次训练和测试，最终返回的是这k 个测试结果 的均值。



##### 2.2.3 自助法

// 略

自助法在数据集较小、难以有效划分训练/测试集时很有用



##### 2.2.4 调参与最终模型

我们通常把学得模型在实际使用中遇到的数据称为测试数据，模型评估与选择中用于评估测试的数据集常称为**“验证集"(validation set)**



把训练数据另外划分 为训练集和验证集,基于验证集上的性能来进行模型选择和调参.





#### 2.3 性能度量

对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需 要有衡量模型泛化能力的评价标准，这就是**性能度量(performance measure)** 

回归任务最常用的性能度量是**均方误差（mean squared error）**



##### 2.3.1 错误率与精度

分类任务中最常用的两种性能度量是错误率与精度



##### 2.3.2 查准率、查全率与F1

// todo



##### 2.3.3 ROC与AUC

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与 一个分类阈值(threshold)（下段中的**截断点**）进行比较，若大于阈值则分为正类，否则为反类

在不同的应用任务中，我们可根据任务需求来采用不同的截断点，例如若 我们更重视“查准率”，则可选择排序中靠前的位置进行截断;若更重视“查 全率”，则可选择靠后的位置进行截断

...



##### 2.3.4 代价敏感错误率与代价曲线

...





#### 2.4 比较检验

统计假设检验(hypothesis test)为我们进行学习器性能比较提供了重要依据.



##### 2.4.1 假设检验



##### 2.4.2 交叉验证t检验



##### 2.4.3 McNemar检验



#### 2.5 偏差与方差

**偏差（bias）** 期望输出与真实标记的差别


